{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbd4eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Utility Functions\n",
    "# \n",
    "# Function to extract the minute and second parts of the timestamp from the filename\n",
    "\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def get_timestamp_from_filename(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.split('_')\n",
    "    hr = int(parts[-3])\n",
    "    minute = int(parts[-2])\n",
    "    second = int(parts[-1].split('.')[0])  # Removing the \".csv\" part\n",
    "    return (hr, minute, second)\n",
    "\n",
    "def eul_2_quat(rpy):\n",
    "    rotation = R.from_rotvec(rpy)\n",
    "    return rotation.as_quat()\n",
    "\n",
    "def quat_2_eul(quat):\n",
    "    rotation = R.from_quat(quat)\n",
    "    return rotation.as_euler('xyz', degrees=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5616466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV File: data/push_task/data/2024_09_19_18_09_11.csv\n",
      "length of demo: 110\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_13_52.csv\n",
      "length of demo: 108\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_16_20.csv\n",
      "length of demo: 74\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_21_37.csv\n",
      "length of demo: 80\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_24_26.csv\n",
      "length of demo: 65\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_26_21.csv\n",
      "length of demo: 71\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_28_04.csv\n",
      "length of demo: 61\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_29_44.csv\n",
      "length of demo: 69\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_31_30.csv\n",
      "length of demo: 76\n",
      "Reading CSV File: data/push_task/data/2024_09_19_18_34_55.csv\n",
      "length of demo: 66\n",
      "----------------------------\n",
      "Total number of demos: 10\n",
      "----------------------------\n",
      "data: dict_keys(['demo_0', 'demo_1', 'demo_2', 'demo_3', 'demo_4', 'demo_5', 'demo_6', 'demo_7', 'demo_8', 'demo_9'])\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV Files - create a dictionary with the data\n",
    "\n",
    "csv_folder = \"data/push_task/data\"\n",
    "data = {}\n",
    "\n",
    "csv_files = glob.glob(os.path.join(csv_folder, '*.csv'))\n",
    "sorted_file_paths = sorted(csv_files, key=get_timestamp_from_filename)\n",
    "# Sort the CSV files based on name\n",
    "# print(sorted_file_paths)\n",
    "for n, csv_file in enumerate(sorted_file_paths):\n",
    "\n",
    "\n",
    "    print(f\"Reading CSV File: {csv_file}\")\n",
    "    timestamps = pd.read_csv(csv_file, usecols=[0], engine=\"python\")\n",
    "    joint_angles = pd.read_csv(csv_file, usecols=range(1, 7), engine=\"python\").astype(np.float64)\n",
    "\n",
    "    \n",
    "    gripper_pos = pd.read_csv(csv_file, usecols=[7], engine=\"python\").astype(np.float64)\n",
    "    gripper_pos = np.array(gripper_pos)\n",
    "    \n",
    "\n",
    "    ee_pos_xyz = pd.read_csv(csv_file, usecols=range(8, 11), engine=\"python\").astype(np.float64)\n",
    "    ee_pos_rpy = pd.read_csv(csv_file, usecols=range(11, 14), engine=\"python\").astype(np.float64)\n",
    "\n",
    "    # Create a nested dictionary for each demonstration\n",
    "    data[f'demo_{n}'] = {\n",
    "        # 'timestamps': np.array(timestamps)[::2],\n",
    "        # 'joint_angles': np.array(joint_angles)[::2],\n",
    "        # 'gripper_pos': np.array(gripper_pos),\n",
    "        # 'ee_pos_xyz': np.array(ee_pos_xyz)[::2],\n",
    "        'timestamps': np.array(timestamps),\n",
    "        'joint_angles': np.array(joint_angles),\n",
    "        'gripper_pos': np.array(gripper_pos),\n",
    "        'ee_pos_xyz': np.array(ee_pos_xyz),\n",
    "        # convert rpy to quaternion\n",
    "        # 'ee_pos_quat': np.array(eul_2_quat(np.array(ee_pos_rpy)[::2])),\n",
    "    }\n",
    "    print(f\"length of demo: {len(data[f'demo_{n}']['gripper_pos'])}\")\n",
    "\n",
    "    # if n == 9:      # Only 10 trajectory\n",
    "    #     break\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(f\"Total number of demos: {len(data)}\")\n",
    "print(\"----------------------------\")\n",
    "print(f\"data: {data.keys()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b88caa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_08_45\n",
      "Number of images: 125\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_13_28\n",
      "Number of images: 143\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_15_49\n",
      "Number of images: 99\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_21_13\n",
      "Number of images: 105\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_24_02\n",
      "Number of images: 87\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_25_54\n",
      "Number of images: 92\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_27_39\n",
      "Number of images: 79\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_29_20\n",
      "Number of images: 82\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_31_06\n",
      "Number of images: 102\n",
      "Reading Image Folder: data/push_task/cam0/2024_09_19_18_34_16\n",
      "Number of images: 86\n",
      "----------------------------------------\n",
      "Total number of images folders: 10\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Read Images [cam0 - Env Camera] folder - Create a dictionary with the images\n",
    "\n",
    "import cv2\n",
    "\n",
    "images = \"\"\n",
    "\n",
    "image_folders = glob.glob(os.path.join(\"data/push_task/cam0\", '*'))\n",
    "sorted_image_folders = sorted(image_folders, key=get_timestamp_from_filename)\n",
    "# Sort the CSV files based on name\n",
    "for n, image_folder in enumerate(sorted_image_folders):\n",
    "    print(f\"Reading Image Folder: {image_folder}\")\n",
    "    image_files = glob.glob(os.path.join(image_folder, '*.png'))\n",
    "    \n",
    "    # Sort the image files based on name\n",
    "    sorted_image_files = sorted(image_files, key=get_timestamp_from_filename)\n",
    "\n",
    "    # print(sorted_image_files)\n",
    "    # get all image files as numpy array\n",
    "    cv_images=[]\n",
    "    for i in range(0, len(sorted_image_files), 1):\n",
    "        cv_image = cv2.imread(sorted_image_files[i])\n",
    "        cv_image = cv2.resize(cv_image, (96, 96))\n",
    "        # Transpose the image to (3, 320, 320)\n",
    "        cv_image = np.transpose(cv_image, (2, 0, 1))\n",
    "        cv_images.append(cv_image)\n",
    "\n",
    "    print(f\"Number of images: {len(cv_images)}\")\n",
    "    # Create a nested dictionary for each demonstration\n",
    "    data[f'demo_{n}']['cam0'] = np.array(cv_images)\n",
    "\n",
    "    # if n == 9:      # Only 10 datasets\n",
    "    #     break\n",
    "\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"Total number of images folders: {len(sorted_image_folders)}\")\n",
    "print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c860c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_08_52\n",
      "Number of images: 126\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_13_32\n",
      "Number of images: 144\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_15_57\n",
      "Number of images: 100\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_21_18\n",
      "Number of images: 106\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_24_07\n",
      "Number of images: 88\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_25_58\n",
      "Number of images: 94\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_27_44\n",
      "Number of images: 81\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_29_24\n",
      "Number of images: 83\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_31_11\n",
      "Number of images: 103\n",
      "Reading Image Folder: data/push_task/cam1/2024_09_19_18_34_21\n",
      "Number of images: 87\n",
      "----------------------------------------\n",
      "Total number of images folders: 10\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Read Images [cam1 - In-Hand Camera] folder - Create a dictionary with the images\n",
    "\n",
    "import cv2\n",
    "\n",
    "images = \"\"\n",
    "\n",
    "image_folders = glob.glob(os.path.join(\"data/push_task/cam1\", '*'))\n",
    "sorted_image_folders = sorted(image_folders, key=get_timestamp_from_filename)\n",
    "# Sort the CSV files based on name\n",
    "for n, image_folder in enumerate(sorted_image_folders):\n",
    "    print(f\"Reading Image Folder: {image_folder}\")\n",
    "    image_files = glob.glob(os.path.join(image_folder, '*.png'))\n",
    "    \n",
    "    # Sort the image files based on name\n",
    "    sorted_image_files = sorted(image_files, key=get_timestamp_from_filename)\n",
    "\n",
    "    # print(sorted_image_files)\n",
    "    # get all image files as numpy array\n",
    "    cv_images=[]\n",
    "    for i in range(0, len(sorted_image_files), 1):\n",
    "        cv_image = cv2.imread(sorted_image_files[i])\n",
    "        # cv_image = cv2.rotate(cv_image, cv2.ROTATE_90_CLOCKWISE)       # Temporary Edit\n",
    "        cv_image = cv2.resize(cv_image, (96, 96))\n",
    "        # Transpose the image to (3, 320, 320)\n",
    "        cv_image = np.transpose(cv_image, (2, 0, 1))\n",
    "        cv_images.append(cv_image)\n",
    "\n",
    "    print(f\"Number of images: {len(cv_images)}\")\n",
    "    # Create a nested dictionary for each demonstration\n",
    "    data[f'demo_{n}']['cam1'] = np.array(cv_images)\n",
    "\n",
    "\n",
    "    # if n == 9:      # Only 10 datasets\n",
    "        # break\n",
    "\n",
    "\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"Total number of images folders: {len(sorted_image_folders)}\")\n",
    "print(\"----------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1ade5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data demo_0: 110 timestamps, cam0: 125, cam1: 126\n",
      "Data demo_1: 108 timestamps, cam0: 143, cam1: 144\n",
      "Data demo_2: 74 timestamps, cam0: 99, cam1: 100\n",
      "Data demo_3: 80 timestamps, cam0: 105, cam1: 106\n",
      "Data demo_4: 65 timestamps, cam0: 87, cam1: 88\n",
      "Data demo_5: 71 timestamps, cam0: 92, cam1: 94\n",
      "Data demo_6: 61 timestamps, cam0: 79, cam1: 81\n",
      "Data demo_7: 69 timestamps, cam0: 82, cam1: 83\n",
      "Data demo_8: 76 timestamps, cam0: 102, cam1: 103\n",
      "Data demo_9: 66 timestamps, cam0: 86, cam1: 87\n"
     ]
    }
   ],
   "source": [
    "# Compare the length of csv files to the number of images in the folder\n",
    "\n",
    "for demo in data:\n",
    "    print(f\"Data {demo}: {len(data[demo]['ee_pos_xyz'])} timestamps, cam0: {len(data[demo]['cam0'])}, cam1: {len(data[demo]['cam1'])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "105f9621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "After matching: Data demo_0: 110 timestamps, cam0: 110, cam1: 110\n",
      "0\n",
      "After matching: Data demo_1: 108 timestamps, cam0: 108, cam1: 108\n",
      "0\n",
      "After matching: Data demo_2: 74 timestamps, cam0: 74, cam1: 74\n",
      "0\n",
      "After matching: Data demo_3: 80 timestamps, cam0: 80, cam1: 80\n",
      "0\n",
      "After matching: Data demo_4: 65 timestamps, cam0: 65, cam1: 65\n",
      "0\n",
      "After matching: Data demo_5: 71 timestamps, cam0: 71, cam1: 71\n",
      "0\n",
      "After matching: Data demo_6: 61 timestamps, cam0: 61, cam1: 61\n",
      "0\n",
      "After matching: Data demo_7: 69 timestamps, cam0: 69, cam1: 69\n",
      "0\n",
      "After matching: Data demo_8: 76 timestamps, cam0: 76, cam1: 76\n",
      "0\n",
      "After matching: Data demo_9: 66 timestamps, cam0: 66, cam1: 66\n"
     ]
    }
   ],
   "source": [
    "# Match the lengths of timestamps and images\n",
    "\n",
    "def match_lengths(data):\n",
    "    for demo in data:\n",
    "        num_timestamps = len(data[demo]['ee_pos_xyz'])\n",
    "        cam0 = len(data[demo]['cam0'])\n",
    "        cam1 = len(data[demo]['cam1'])\n",
    "\n",
    "        get_min_ind = np.argmin(np.array([num_timestamps, cam0, cam1]))\n",
    "        print(get_min_ind)\n",
    "\n",
    "\n",
    "        if get_min_ind == 0:\n",
    "            # timestamps are less - remove cam0 and cam1\n",
    "            excess_cam0 = cam0 - num_timestamps\n",
    "            data[demo]['cam0'] = data[demo]['cam0'][excess_cam0:]\n",
    "\n",
    "            excess_cam1 = cam1 - num_timestamps\n",
    "            data[demo]['cam1'] = data[demo]['cam1'][excess_cam1:]\n",
    "        elif get_min_ind == 1:\n",
    "            # cam0 is less - remove timestamps and cam1\n",
    "            excess_timestamps = num_timestamps - cam0\n",
    "            for i in data[demo].keys():\n",
    "                if i == \"cam0\" or i == \"cam1\":\n",
    "                    continue\n",
    "                data[demo][i] = data[demo][i][excess_timestamps:]\n",
    "\n",
    "            excess_cam1 = cam1 - cam0\n",
    "            data[demo]['cam1'] = data[demo]['cam1'][excess_cam1:]\n",
    "            \n",
    "        elif get_min_ind == 2:\n",
    "            # cam1 is less - remove timestamps and cam0\n",
    "            excess_timestamps = num_timestamps - cam1\n",
    "            for i in data[demo].keys():\n",
    "                if i == \"cam1\" or i == \"cam0\":\n",
    "                    continue\n",
    "                data[demo][i] = data[demo][i][excess_timestamps:]\n",
    "\n",
    "            excess_cam0 = cam0 - cam1\n",
    "            data[demo]['cam0'] = data[demo]['cam0'][excess_cam0:]\n",
    "\n",
    "\n",
    "        \n",
    "        # # ---- For cam0 [Env]-----\n",
    "        # if cam0 > num_timestamps:\n",
    "        #     # Calculate the number of excess images\n",
    "        #     excess_images = cam0 - num_timestamps\n",
    "            \n",
    "        #     # Remove excess images from the beginning\n",
    "        #     data[demo]['cam0'] = data[demo]['cam0'][excess_images:]\n",
    "        # elif cam0 < num_timestamps:\n",
    "        #     # Calucate the number of exxess timestamps\n",
    "        #     excess_timestamps = num_timestamps - cam0\n",
    "            \n",
    "        #     # Remove excess timestamps from the end\n",
    "        #     for i in data[demo].keys():\n",
    "        #         if i == \"cam0\":\n",
    "        #             pass\n",
    "        #         data[demo][i] = data[demo][i][excess_timestamps:]\n",
    "\n",
    "        # # ---- For cam1 [In-Hand]-----\n",
    "        # if cam1 > num_timestamps:\n",
    "        #     # Calculate the number of excess images\n",
    "        #     excess_images = cam1 - num_timestamps\n",
    "            \n",
    "        #     # Remove excess images from the beginning\n",
    "        #     data[demo]['cam1'] = data[demo]['cam1'][excess_images:]\n",
    "        # elif cam1 < num_timestamps:\n",
    "        #     # Calucate the number of exxess timestamps\n",
    "        #     excess_timestamps = num_timestamps - cam1\n",
    "            \n",
    "        #     # Remove excess timestamps from the end\n",
    "        #     for i in data[demo].keys():\n",
    "        #         if i == \"cam1\":\n",
    "        #             pass\n",
    "        #         data[demo][i] = data[demo][i][excess_timestamps:]\n",
    "        \n",
    "        print(f\"After matching: Data {demo}: {len(data[demo]['ee_pos_xyz'])} timestamps, cam0: {len(data[demo]['cam0'])}, cam1: {len(data[demo]['cam1'])}\")\n",
    "\n",
    "# Example usage\n",
    "match_lengths(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8042d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Preprocess the Gripper Position\n",
    "\n",
    "# # indicies = np.array([44, 44, 43, 36, 44, 40, 34, 42, 48, 40])\n",
    "# # print(indicies)\n",
    "\n",
    "# # for n, i in enumerate(data.keys()):\n",
    "# #     gripper_pos = data[i]['gripper_pos']\n",
    "\n",
    "# #     # -------------- Change gripper pos at corresponding index --------------------\n",
    "# #     gripper_pos[:indicies[n]] = 0.0\n",
    "# #     gripper_pos[indicies[n]:] = 0.5\n",
    "\n",
    "# #     data[i]['gripper_pos'] = gripper_pos\n",
    "\n",
    "# # print(\"Gripper Poses Updated\")\n",
    "\n",
    "\n",
    "# ## Approximate gripper position - 0.6\n",
    "\n",
    "# for demo in data.keys():\n",
    "#     gripper_pos = data[demo]['gripper_pos']\n",
    "#     gripper_pos_new = np.where(gripper_pos > 0.1, 0.6, 0.0)\n",
    "#     data[demo]['gripper_pos'] = gripper_pos_new \n",
    "    \n",
    "# print(\"--------------------------------------\")\n",
    "# print(\"Gripper Pos set to 0.0 or 0.6\")\n",
    "# print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6407fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions Added: Dimension: (109, 4)\n",
      "Actions Added: Dimension: (107, 4)\n",
      "Actions Added: Dimension: (73, 4)\n",
      "Actions Added: Dimension: (79, 4)\n",
      "Actions Added: Dimension: (64, 4)\n",
      "Actions Added: Dimension: (70, 4)\n",
      "Actions Added: Dimension: (60, 4)\n",
      "Actions Added: Dimension: (68, 4)\n",
      "Actions Added: Dimension: (75, 4)\n",
      "Actions Added: Dimension: (65, 4)\n"
     ]
    }
   ],
   "source": [
    "# Metaworld Dataset - HDF5\n",
    "# \n",
    "# Actions - Difference (next - current)\n",
    "\n",
    "\n",
    "for demo in data.keys():\n",
    "    demo_data = data[demo]\n",
    "    timestamps = demo_data['timestamps']\n",
    "    ee_pos_xyz = demo_data['ee_pos_xyz']\n",
    "    # ee_pos_quat = demo_data['ee_pos_quat']\n",
    "    gripper_pos = demo_data['gripper_pos']\n",
    "    cam0 = demo_data['cam0']\n",
    "    cam1 = demo_data['cam1']\n",
    "\n",
    "    # ee_pos = np.concatenate((ee_pos_xyz, ee_pos_quat), axis=1)\n",
    "    # actions = ee_pos[1:] - ee_pos[:-1]\n",
    "    actions = ee_pos_xyz[1:] - ee_pos_xyz[:-1]\n",
    "\n",
    "    # actions concatenate gripper_pos\n",
    "    actions = np.concatenate((actions, gripper_pos[:-1]), axis=1)\n",
    "    data[demo][\"actions\"] = actions\n",
    "\n",
    "    print(f\"Actions Added: Dimension: {data[demo]['actions'].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a5cd1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_pos: [-0.13156664 -0.29868215  0.13892271  0.        ]\n",
      "[-0.1315755  -0.29867592  0.13891415  0.97647059]\n",
      "[-0.13157829 -0.29868213  0.13892212  0.97647059]\n",
      "[-0.13157545 -0.2986816   0.13891919  0.97647059]\n",
      "[-0.13156216 -0.29866601  0.13890684  0.97647059]\n",
      "[-0.13159122 -0.29868046  0.1355753   0.97647059]\n",
      "[-0.13157661 -0.29869771  0.13282828  0.97647059]\n",
      "[-0.13158645 -0.29869324  0.13019808  0.97647059]\n",
      "[-0.13159726 -0.29873024  0.12331053  0.97647059]\n",
      "[-0.13157956 -0.29876661  0.11785576  0.97647059]\n",
      "[-0.12900118 -0.29875361  0.11268324  0.97647059]\n",
      "[-0.12529046 -0.29880881  0.1055784   0.97647059]\n",
      "[-0.12259347 -0.29879958  0.10016354  0.97647059]\n",
      "[-0.11891834 -0.29886077  0.09289456  0.97647059]\n",
      "[-0.1161137  -0.29884411  0.08723928  0.97647059]\n",
      "[-0.1135367  -0.2988539   0.08200964  0.97647059]\n",
      "[-0.11316951 -0.29890079  0.07482324  0.97647059]\n",
      "[-0.11316234 -0.29900134  0.06939508  0.97647059]\n",
      "[-0.11314367 -0.29896604  0.06422421  0.97647059]\n",
      "[-0.11314252 -0.2990449   0.05712804  0.97647059]\n",
      "[-0.1131581  -0.29902157  0.05174412  0.97647059]\n",
      "[-0.11314472 -0.29909267  0.04596901  0.97647059]\n",
      "[-0.11312094 -0.29909775  0.03879385  0.97647059]\n",
      "[-0.11316184 -0.29911487  0.03355338  0.97647059]\n",
      "[-0.11314174 -0.29917669  0.02635512  0.97647059]\n",
      "[-0.11313881 -0.29922584  0.02091656  0.97647059]\n",
      "[-0.11313186 -0.29934482  0.01510076  0.97647059]\n",
      "[-0.11312909 -0.29933596  0.00783315  0.97647059]\n",
      "[-0.11312606 -0.29934857  0.00247169  0.97647059]\n",
      "[-0.1131345  -0.2994003  -0.00482561  0.97647059]\n",
      "[-0.11311856 -0.29932984 -0.00443063  0.97647059]\n",
      "[-0.11311483 -0.29938942 -0.00438571  0.97647059]\n",
      "[-0.11313305 -0.29935426 -0.0045223   0.97647059]\n",
      "[-0.11312141 -0.30238594 -0.00443336  0.97647059]\n",
      "[-0.11311651 -0.30546828 -0.00444973  0.97647059]\n",
      "[-0.1131043  -0.30905813 -0.00445323  0.97647059]\n",
      "[-0.11310969 -0.31171952 -0.00446058  0.97647059]\n",
      "[-0.11309998 -0.31541247 -0.00443884  0.97647059]\n",
      "[-0.11308334 -0.31815467 -0.00446779  0.97647059]\n",
      "[-0.11309762 -0.32076081 -0.00445661  0.97647059]\n",
      "[-0.11307692 -0.32426645 -0.00448937  0.97647059]\n",
      "[-0.11310384 -0.32695585 -0.00451198  0.97647059]\n",
      "[-0.11309314 -0.32983786 -0.00452337  0.97647059]\n",
      "[-0.11000178 -0.33342496 -0.00455867  0.97647059]\n",
      "[-0.10737513 -0.33608477 -0.00454062  0.97647059]\n",
      "[-0.10706933 -0.33973108 -0.00452695  0.97647059]\n",
      "[-0.10705863 -0.34248145 -0.00453769  0.97647059]\n",
      "[-0.10704258 -0.34500266 -0.00459401  0.97647059]\n",
      "[-0.10703493 -0.34863456 -0.00454597  0.97647059]\n",
      "[-0.10701263 -0.3513531  -0.00454717  0.97647059]\n",
      "[-0.10700016 -0.35429352 -0.0045224   0.97647059]\n",
      "[-0.10700497 -0.35790786 -0.00451511  0.97647059]\n",
      "[-0.10698984 -0.36055465 -0.0045171   0.97647059]\n",
      "[-0.10697567 -0.36422331 -0.00449976  0.97647059]\n",
      "[-0.10698876 -0.36700713 -0.00447979  0.97647059]\n",
      "[-0.10699039 -0.36954992 -0.00451757  0.97647059]\n",
      "[-0.10697166 -0.37316572 -0.00448396  0.97647059]\n",
      "[-0.10695632 -0.37592331 -0.00443516  0.97647059]\n",
      "[-0.10695415 -0.37874442 -0.00452319  0.97647059]\n",
      "[-0.1069621  -0.38233015 -0.00453532  0.97647059]\n",
      "[-0.10694787 -0.38501149 -0.00450465  0.97647059]\n",
      "[-0.10695805 -0.38865982 -0.00450412  0.97647059]\n",
      "[-0.10694193 -0.39151595 -0.00446398  0.97647059]\n",
      "[-0.10692964 -0.39408088 -0.00445081  0.97647059]\n",
      "[-0.10690674 -0.39762398 -0.00445137  0.97647059]\n",
      "[-0.10694    -0.40024015 -0.00455906  0.97647059]\n",
      "[-0.10693039 -0.40322279 -0.00449965  0.97647059]\n",
      "[-0.10693491 -0.40675537 -0.00456031  0.97647059]\n",
      "[-0.10694532 -0.40943891 -0.00452032  0.97647059]\n",
      "[-0.10694504 -0.41305875 -0.00453935  0.97647059]\n",
      "[-0.10693008 -0.41580031 -0.00454139  0.97647059]\n",
      "[-0.10695386 -0.41833889 -0.00453965  0.97647059]\n",
      "[-0.10695466 -0.42192342 -0.00451276  0.97647059]\n",
      "[-0.10694414 -0.42455054 -0.00454663  0.97647059]\n",
      "[-0.10695085 -0.42743235 -0.00454685  0.97647059]\n",
      "[-0.10692578 -0.43101898 -0.00457199  0.97647059]\n",
      "[-0.1069369  -0.4336001  -0.00459226  0.97647059]\n",
      "[-0.10692946 -0.43721198 -0.00459177  0.97647059]\n",
      "[-0.10692141 -0.43988884 -0.0046152   0.97647059]\n",
      "[-0.1069222  -0.44287657 -0.00453432  0.97647059]\n",
      "[-0.10690875 -0.44647287 -0.00455939  0.97647059]\n",
      "[-0.10687741 -0.44909285 -0.00459589  0.97647059]\n",
      "[-0.1068661  -0.45199857 -0.00462228  0.97647059]\n",
      "[-0.10685573 -0.45279649 -0.00455765  0.97647059]\n",
      "[-0.1068638  -0.45294005 -0.0046494   0.97647059]\n",
      "[-0.10685024 -0.45298779 -0.00464431  0.97647059]\n",
      "[-0.10685193 -0.45299437 -0.00464459  0.97647059]\n",
      "[-0.10685687 -0.45299592 -0.0046508   0.97647059]\n",
      "[-0.10686308 -0.45300081 -0.00464064  0.97647059]\n",
      "[-0.10686713 -0.45299366 -0.00464872  0.97647059]\n",
      "[-0.10683334 -0.45299035 -0.00465238  0.97647059]\n",
      "[-0.10686383 -0.45299639 -0.00464881  0.97647059]\n",
      "[-0.10687523 -0.45298887 -0.00465526  0.97647059]\n",
      "[-0.10684286 -0.45297649 -0.00466318  0.97647059]\n",
      "[-0.10685623 -0.45299024 -0.00465318  0.97647059]\n",
      "[-0.10683154 -0.45299554 -0.00464907  0.97647059]\n",
      "[-0.10686282 -0.45297828 -0.0046587   0.97647059]\n",
      "[-0.10685857 -0.45298382 -0.00465669  0.97647059]\n",
      "[-0.10686747 -0.45297763 -0.00465428  0.97647059]\n",
      "[-0.10685823 -0.45297535 -0.00466241  0.97647059]\n",
      "[-0.10686105 -0.4529804  -0.00465807  0.97647059]\n",
      "[-0.10685529 -0.45297775 -0.00465354  0.97647059]\n",
      "[-0.10685588 -0.45297288 -0.00466506  0.97647059]\n",
      "[-0.10685237 -0.45296823 -0.00466052  0.97647059]\n",
      "[-0.10684904 -0.45295985 -0.00467487  0.97647059]\n",
      "[-0.10685703 -0.45295896 -0.00467179  0.97647059]\n",
      "[-0.1068704  -0.45297873 -0.004669    0.97647059]\n",
      "[-0.10684641 -0.45297346 -0.00466508  0.97647059]\n",
      "[-0.10684973 -0.45297292 -0.00466575  0.97647059]\n",
      "[-0.10684833 -0.45296941 -0.00467187  0.97647059]\n"
     ]
    }
   ],
   "source": [
    "# check the actions\n",
    "# Concatenate gripper_pos\n",
    "first_pos = data['demo_0']['ee_pos_xyz'][0]\n",
    "# gripper_pos = d[0]\n",
    "first_pos = np.concatenate((first_pos, [0]), axis=0)\n",
    "\n",
    "print(f\"first_pos: {first_pos}\")\n",
    "\n",
    "# Add actions\n",
    "traj = []\n",
    "check_actions = data[\"demo_0\"][\"actions\"]\n",
    "for i in check_actions:\n",
    "    first_pos[:3]+=i[:3]\n",
    "    first_pos[3:] = i[3:]\n",
    "    print(first_pos)\n",
    "# print(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfa62260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "Dataset - hdf5 Generated.. Wohoooo!\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Lets build HDF5 - Metaworld Dataset\n",
    "\n",
    "\n",
    "import h5py\n",
    "\n",
    "with h5py.File('data.hdf5', 'w') as f:\n",
    "\n",
    "\n",
    "    data_group = f.create_group(\"data\")\n",
    "    \n",
    "\n",
    "\n",
    "    for demo in data.keys():\n",
    "\n",
    "        demo_group = data_group.create_group(demo)\n",
    "\n",
    "        # Create action\n",
    "        demo_group.create_dataset(\"actions\", data=data[demo][\"actions\"])\n",
    "\n",
    "\n",
    "        # Create dones\n",
    "        dones = np.zeros(len(data[demo][\"actions\"]))\n",
    "        dones[-1] = 1\n",
    "        demo_group.create_dataset(\"dones\", data=dones)\n",
    "\n",
    "\n",
    "        # Create obs group\n",
    "        obs_group = demo_group.create_group(\"obs\")\n",
    "        # Create eye_in_hand_image\n",
    "        obs_group.create_dataset(\"eye_in_hand_image\", data=data[demo][\"cam0\"][:-1])\n",
    "        obs_group.create_dataset(\"front_image\", data=data[demo][\"cam1\"][:-1])\n",
    "        # Create prop\n",
    "        prop = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"gripper_pos\"]), axis=1)\n",
    "        obs_group.create_dataset(\"prop\", data=prop[:-1])\n",
    "\n",
    "\n",
    "        # Create rewards\n",
    "        rewards = np.zeros(len(data[demo][\"actions\"]))\n",
    "        rewards[-1] = 1\n",
    "        demo_group.create_dataset(\"rewards\", data=rewards)\n",
    "\n",
    "\n",
    "        # Create states\n",
    "        demo_group.create_dataset(\"states\", data=prop[:-1])\n",
    "\n",
    "print(\"----------------------------\")\n",
    "print(\"Dataset - hdf5 Generated.. Wohoooo!\")\n",
    "print(\"----------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5024467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets build dataset - Robosuite Dataset\n",
    "# # \n",
    "# # 1. -- Ignored attributes from default dataset --\n",
    "# #       obs - [object, robot0_eef_vel_ang, robot0_eef_vel_lin, robot0_gripper_qvel, \n",
    "# #               robot0_joint_pos_cos, robot0_joint_pos_sin, robot0_joint_vel_cos, \n",
    "# #               robot0_joint_vel_sin, robot0_joint_vel]\n",
    "# # \n",
    "# # 2. Skipped some columns in of states\n",
    "\n",
    "# import h5py\n",
    "\n",
    "# with h5py.File('data/dataset_2024_7_19/data_robosuite.hdf5', 'w') as f:\n",
    "\n",
    "#     data_group_r = f.create_group(\"data\")\n",
    "\n",
    "#     for demo in data.keys():\n",
    "        \n",
    "#         demo_group_r = data_group_r.create_group(demo)\n",
    "\n",
    "#         # Create actions\n",
    "#         demo_group_r.create_dataset(\"actions\", data=data[demo][\"actions\"])\n",
    "\n",
    "#         # Create dones\n",
    "#         dones_r = np.zeros(len(data[demo][\"actions\"]))\n",
    "#         dones_r[-1] = 1\n",
    "#         demo_group_r.create_dataset(\"dones\", data=dones_r)\n",
    "\n",
    "#         # Create obs group\n",
    "#         obs_group_r = demo_group_r.create_group(\"obs\")\n",
    "#         # Create agentview_image\n",
    "#         obs_group_r.create_dataset(\"agentview_image\", data=data[demo][\"images\"][:-1])\n",
    "#         # Create robot0_eef_pos\n",
    "#         obs_group_r.create_dataset(\"robot0_eef_pos\", data=data[demo][\"ee_pos_rpy\"][:, :3][:-1])\n",
    "#         # Create robot0_eef_rpy\n",
    "#         obs_group_r.create_dataset(\"robot0_eef_rpy\", data=data[demo][\"ee_pos_rpy\"][:, 3:][:-1])\n",
    "#         # Create robot0_gripper_qpos\n",
    "#         obs_group_r.create_dataset(\"robot0_gripper_qpos\", data=data[demo][\"gripper_pos\"][:-1])\n",
    "#         # Create robot0_joint_pos\n",
    "#         obs_group_r.create_dataset(\"robot0_joint_pos\", data=data[demo][\"joint_angles\"][:-1])\n",
    "\n",
    "#         # Create rewards\n",
    "#         rewards_r = np.zeros(len(data[demo][\"actions\"]))\n",
    "#         rewards_r[-1] = 1\n",
    "#         demo_group_r.create_dataset(\"rewards\", data=rewards_r)\n",
    "\n",
    "#         # Create states\n",
    "#         # [time, joint_angles, gripper_pos, ee_pos_rpy, first_obj_pose, second_obj_pose]\n",
    "#         states = np.concatenate((data[demo][\"joint_angles\"], data[demo][\"gripper_pos\"], data[demo][\"ee_pos_rpy\"]), axis=1)\n",
    "#         demo_group_r.create_dataset(\"states\", data=states[:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e1b7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset - Metaworld [Bad Data - demo1 and demo6]\n",
    "\n",
    "\n",
    "# # def change_gripper_pos(gripper_pos_array):\n",
    "# #     indices = [46, 49, 47, 47, 49, 43, 41, 50]\n",
    "# #     gripper_pos = np.zeros()\n",
    "\n",
    "\n",
    "\n",
    "# import h5py\n",
    "\n",
    "# with h5py.File('data/dataset_2024_7_25_2/data_processed_new.hdf5', 'w') as f:\n",
    "\n",
    "\n",
    "#     data_group_b = f.create_group(\"data\")\n",
    "\n",
    "#     indices = [46, 49, 47, 47, 49, 43, 41, 50]\n",
    "\n",
    "\n",
    "#     n = 0\n",
    "#     for demo in data.keys():\n",
    "\n",
    "#         if demo == \"demo_5\":\n",
    "#             continue\n",
    "\n",
    "#         demo_group_b = data_group_b.create_group(f\"demo_{n}\")\n",
    "\n",
    "#         # Create action\n",
    "#         demo_group_b.create_dataset(\"actions\", data=data[demo][\"actions\"])\n",
    "\n",
    "#         # Create dones\n",
    "#         dones_b = np.zeros(len(data[demo][\"actions\"]))\n",
    "#         dones_b[-1] = 1\n",
    "#         demo_group_b.create_dataset(\"dones\", data=dones_b)\n",
    "\n",
    "\n",
    "#         # Create obs group\n",
    "#         obs_group_b = demo_group_b.create_group(\"obs\")\n",
    "#         # Create corner2_image\n",
    "#         obs_group_b.create_dataset(\"eye_in_hand_image\", data=data[demo][\"cam0\"][:-1])\n",
    "#         obs_group_b.create_dataset(\"front_image\", data=data[demo][\"cam1\"][:-1])\n",
    "#         # obs_group_b.create_dataset(\"eye_in_hand_image\", data=data[demo][\"images\"][:-1])\n",
    "#         # Create prop\n",
    "#         # ee_pos = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"ee_pos_quat\"]), axis=1)\n",
    "#         prop_b = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"gripper_pos\"]), axis=1)\n",
    "#         obs_group_b.create_dataset(\"prop\", data=prop_b[:-1])\n",
    "\n",
    "\n",
    "#         # Create rewards\n",
    "#         rewards_b = np.zeros(len(data[demo][\"actions\"]))\n",
    "#         rewards_b[-1] = 1\n",
    "#         demo_group_b.create_dataset(\"rewards\", data=rewards_b)\n",
    "\n",
    "\n",
    "#         # Create states\n",
    "#         states_b = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"gripper_pos\"]), axis=1)\n",
    "#         demo_group_b.create_dataset(\"states\", data=states_b[:-1])\n",
    "\n",
    "#         n+=1\n",
    "\n",
    "# print(\"----------------------------\")\n",
    "# print(\"Processed Dataset - hdf5 Generated\")\n",
    "# print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2334cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## --- remove some points in the dataset -----------\n",
    "\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "# with h5py.File('data/dataset_2024_07_25/data_removed_lift.hdf5', 'w') as f:\n",
    "    \n",
    "#     data_group_b = f.create_group(\"data\")\n",
    "\n",
    "#     indices = [46, 49, 47, 47, 49, 43, 41, 50]\n",
    "\n",
    "\n",
    "#     n = 0\n",
    "#     for demo in data.keys():\n",
    "\n",
    "#         if demo == \"demo_1\":\n",
    "#             continue\n",
    "\n",
    "#         demo_group_b = data_group_b.create_group(f\"demo_{n}\")\n",
    "\n",
    "#         # Create action\n",
    "#         demo_group_b.create_dataset(\"actions\", data=data[demo][\"actions\"][:indices[n]+1])\n",
    "\n",
    "#         # Create dones\n",
    "#         dones_b = np.zeros(len(data[demo][\"actions\"][:indices[n]+1]))\n",
    "#         dones_b[-1] = 1\n",
    "#         demo_group_b.create_dataset(\"dones\", data=dones_b)\n",
    "\n",
    "\n",
    "#         # Create obs group\n",
    "#         obs_group_b = demo_group_b.create_group(\"obs\")\n",
    "#         # Create corner2_image\n",
    "#         obs_group_b.create_dataset(\"corner2_image\", data=data[demo][\"cam0\"][:indices[n]+1])\n",
    "#         obs_group_b.create_dataset(\"front_image\", data=data[demo][\"cam1\"][:indices[n]+1])\n",
    "#         # obs_group_b.create_dataset(\"corner2_image\", data=data[demo][\"images\"][:-1])\n",
    "#         # Create prop\n",
    "#         # ee_pos = np.concatenate((data[demo][\"ee_pos_xyz\"], data[demo][\"ee_pos_quat\"]), axis=1)\n",
    "#         prop_b = np.concatenate((data[demo][\"ee_pos_xyz\"][:indices[n]+1], data[demo][\"gripper_pos\"][:indices[n]+1]), axis=1)\n",
    "#         obs_group_b.create_dataset(\"prop\", data=prop_b)\n",
    "\n",
    "\n",
    "#         # Create rewards\n",
    "#         rewards_b = np.zeros(len(data[demo][\"actions\"][:indices[n]+1]))\n",
    "#         rewards_b[-1] = 1\n",
    "#         demo_group_b.create_dataset(\"rewards\", data=rewards_b)\n",
    "\n",
    "\n",
    "#         # Create states\n",
    "#         states_b = np.concatenate((data[demo][\"ee_pos_xyz\"][:indices[n]+1], data[demo][\"gripper_pos\"][:indices[n]+1]), axis=1)\n",
    "#         demo_group_b.create_dataset(\"states\", data=states_b)\n",
    "\n",
    "#         n+=1\n",
    "\n",
    "# print(\"----------------------------\")\n",
    "# print(\"Removed Dataset - hdf5 Generated\")\n",
    "# print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get (min, max) of x, y, z\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "with h5py.File('data/dataset_2024_07_25/datanew.hdf5', 'r') as f:\n",
    "    \n",
    "    data_group_b = f[\"data\"]\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    all_z = []\n",
    "    for demo in data_group_b:\n",
    "        states = data_group_b[demo][\"states\"][:]\n",
    "        x = states[:,0]\n",
    "        y = states[:,1]\n",
    "        z = states[:,2]\n",
    "        # w_ = states[:,3]\n",
    "        # x_ = states[:,4]\n",
    "        # y_ = states[:,5]\n",
    "        # z_ = states[:,6]\n",
    "        all_x.append(x)\n",
    "        all_y.append(y)\n",
    "        all_z.append(z)\n",
    "\n",
    "    all_x = np.concatenate(all_x)\n",
    "    all_y = np.concatenate(all_y)\n",
    "    all_z = np.concatenate(all_z)\n",
    "    \n",
    "    print(f\"min x: {np.round(np.min(all_x), 3)}\")\n",
    "    print(f\"min y: {np.round(np.min(all_y), 3)}\")\n",
    "    print(f\"min z: {np.round(np.min(all_z), 3)}\")\n",
    "    print(f\"max x: {np.round(np.max(all_x), 3)}\")\n",
    "    print(f\"max y: {np.round(np.max(all_y), 3)}\")\n",
    "    print(f\"max z: {np.round(np.max(all_z), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af7ea2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset Created! Wohoooooo!!\n"
     ]
    }
   ],
   "source": [
    "# # Read HDF5 - HYRL Dataset Creation - Spase -> Dense\n",
    "\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# with h5py.File('/home/amisha/HYBRID-RL/data/2S_2024_07_29/data_2S.hdf5', 'r') as f1:\n",
    "\n",
    "#     with h5py.File('/home/amisha/HYBRID-RL/data/2S_2024_07_29/data_2S_hyrl.hdf5', 'w') as f2:\n",
    "\n",
    "    \n",
    "#         # indicies = [44, 44, 43, 36, 44, 40, 34, 42, 48, 40]\n",
    "        \n",
    "#         data_group = f1[\"data\"]\n",
    "        \n",
    "#         n_data = f2.create_group(\"data\")\n",
    "\n",
    "#         for n, demo in enumerate(data_group):\n",
    "\n",
    "            \n",
    "\n",
    "#             # -------------- Change gripper pos at corresponding index --------------------\n",
    "#             # actions = data_group[demo][\"actions\"][:]\n",
    "#             # actions[:, 3][:indicies[n]] = 0.0\n",
    "#             # actions[:, 3][indicies[n]:] = 1.0\n",
    "\n",
    "#             # prop = data_group[demo][\"obs\"][\"prop\"][:]\n",
    "#             # prop[:, 3][:indicies[n]] = 0.0\n",
    "#             # prop[:, 3][indicies[n]:] = 1.0\n",
    "\n",
    "\n",
    "#             # states = data_group[demo][\"states\"][:]\n",
    "#             # states[:, 3][:indicies[n]] = 0.0\n",
    "#             # states[:, 3][indicies[n]:] = 1.0\n",
    "\n",
    "\n",
    "#             ## --------------------- Copy same data into new file ------------------------- ##\n",
    "\n",
    "#             n_demo = n_data.create_group(demo)\n",
    "\n",
    "#             # Create action\n",
    "#             n_demo.create_dataset(\"actions\", data=actions)\n",
    "\n",
    "\n",
    "#             # Create dones\n",
    "#             n_demo.create_dataset(\"dones\", data=data_group[demo][\"dones\"])\n",
    "\n",
    "\n",
    "#             # Create obs group\n",
    "#             n_obs = n_demo.create_group(\"obs\")\n",
    "#             # Create corner2_image\n",
    "#             n_obs.create_dataset(\"corner2_image\", data=data_group[demo][\"obs\"][\"corner2_image\"])\n",
    "#             n_obs.create_dataset(\"eye_in_hand_image\", data=data_group[demo][\"obs\"][\"eye_in_hand_image\"])\n",
    "#             # Create prop\n",
    "#             n_obs.create_dataset(\"prop\", data=prop)\n",
    "\n",
    "\n",
    "#             # Create rewards\n",
    "#             n_demo.create_dataset(\"rewards\", data=data_group[demo][\"rewards\"])\n",
    "\n",
    "\n",
    "#             # Create states\n",
    "#             n_demo.create_dataset(\"states\", data=data_group[demo][\"states\"])\n",
    "\n",
    "#             ## ----------------------- New waypoint key ----------------------------------- ##\n",
    "#             # position = indicies[n]-7\n",
    "#             all_points = data_group[demo][\"states\"][:]\n",
    "#             index = np.where(all_points[:, -1] == 0.5)[0][0]\n",
    "#             position = int(index) - 7       # Adjust the value here\n",
    "#             waypoint = np.array([all_points[position][:3]])\n",
    "#             extended_waypoint = np.tile(waypoint, (len(all_points), 1))\n",
    "\n",
    "\n",
    "#             n_demo.create_dataset(\"waypoint\", data=extended_waypoint)\n",
    "\n",
    "#             ## ----------------------- New mode key ----------------------------------- ##\n",
    "#             mode = np.zeros((len(all_points), 1))\n",
    "#             mode[position:] = 1.0\n",
    "\n",
    "#             n_demo.create_dataset(\"mode\", data=mode)\n",
    "\n",
    "\n",
    "# print(\"New Dataset Created! Wohoooooo!!\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4adcbeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset Created! Wohoooooo!!\n"
     ]
    }
   ],
   "source": [
    "# Read HDF5 - HYRL Dataset Creation - Sparse -> Dense -> Sparse -> Dense [2S Dataset]\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with h5py.File('/home/amisha/HYBRID-RL/data/2S_2024_07_29/data_2S.hdf5', 'r') as f1:\n",
    "\n",
    "    with h5py.File('/home/amisha/HYBRID-RL/data/2S_2024_07_29/data_2S_hyrl_SDSSD.hdf5', 'w') as f2:\n",
    "\n",
    "    \n",
    "        # indicies = [44, 44, 43, 36, 44, 40, 34, 42, 48, 40]\n",
    "        \n",
    "        data_group = f1[\"data\"]\n",
    "        \n",
    "        n_data = f2.create_group(\"data\")\n",
    "\n",
    "        for n, demo in enumerate(data_group):\n",
    "\n",
    "            \n",
    "\n",
    "            # -------------- Change gripper pos at corresponding index --------------------\n",
    "            # actions = data_group[demo][\"actions\"][:]\n",
    "            # actions[:, 3][:indicies[n]] = 0.0\n",
    "            # actions[:, 3][indicies[n]:] = 1.0\n",
    "\n",
    "            # prop = data_group[demo][\"obs\"][\"prop\"][:]\n",
    "            # prop[:, 3][:indicies[n]] = 0.0\n",
    "            # prop[:, 3][indicies[n]:] = 1.0\n",
    "\n",
    "\n",
    "            # states = data_group[demo][\"states\"][:]\n",
    "            # states[:, 3][:indicies[n]] = 0.0\n",
    "            # states[:, 3][indicies[n]:] = 1.0\n",
    "\n",
    "\n",
    "            ## --------------------- Copy same data into new file ------------------------- ##\n",
    "\n",
    "            n_demo = n_data.create_group(demo)\n",
    "\n",
    "            # Create action\n",
    "            n_demo.create_dataset(\"actions\", data=data_group[demo][\"actions\"])\n",
    "\n",
    "\n",
    "            # Create dones\n",
    "            n_demo.create_dataset(\"dones\", data=data_group[demo][\"dones\"])\n",
    "\n",
    "\n",
    "            # Create obs group\n",
    "            n_obs = n_demo.create_group(\"obs\")\n",
    "            # Create eye_in_hand_image\n",
    "            n_obs.create_dataset(\"eye_in_hand_image\", data=data_group[demo][\"obs\"][\"eye_in_hand_image\"])\n",
    "            n_obs.create_dataset(\"front_image\", data=data_group[demo][\"obs\"][\"front_image\"])\n",
    "            # Create prop\n",
    "            n_obs.create_dataset(\"prop\", data=data_group[demo][\"obs\"][\"prop\"])\n",
    "\n",
    "\n",
    "            # Create rewards\n",
    "            n_demo.create_dataset(\"rewards\", data=data_group[demo][\"rewards\"])\n",
    "\n",
    "\n",
    "            # Create states\n",
    "            n_demo.create_dataset(\"states\", data=data_group[demo][\"states\"])\n",
    "\n",
    "            ## ----------------------- New waypoint key - 1 ----------------------------------- ##\n",
    "            # # Waypoint 1\n",
    "            all_points = data_group[demo][\"states\"][:]\n",
    "            index1 = np.where((all_points[:, 2] <= 0.025) & (all_points[:, -1] == 0.0))[0]\n",
    "            position1 = index1[0]       # Adjust the value here\n",
    "            waypoint1 = np.array([all_points[position1][:3]])\n",
    "            # print(f\"Waypoint1: {waypoint1}\")\n",
    "            waypoint1 = np.array([-0.04714026, -0.29906368,  0.02476449])\n",
    "            extended_waypoint1 = np.tile(waypoint1, (position1+1, 1))\n",
    "\n",
    "            temp_dense_points = np.zeros((len(all_points) - position1 - 1, extended_waypoint1.shape[1]))\n",
    "            extended_waypoint = np.concatenate((extended_waypoint1, temp_dense_points), axis=0)\n",
    "            \n",
    "            # # Waypoint 2\n",
    "            index2 = np.where(all_points[:, 2] >= 0.14)[0]\n",
    "            position2 = index2[0]\n",
    "            start_index = np.where((all_points[:, 2] >= 0.04) & (all_points[:, -1] == 0.5))[0][0]\n",
    "\n",
    "            waypoint2 = np.array([all_points[position2][:3]])\n",
    "            # print(f\"Waypoint2: {waypoint2}\")\n",
    "            waypoint2 = np.array([-0.04694496, -0.29975165,  0.14389902])\n",
    "            extended_waypoint2 = np.tile(waypoint2, (position2 - start_index + 1, 1))\n",
    "            extended_waypoint[start_index:position2+1] = extended_waypoint2\n",
    "\n",
    "            # # Waypoint 3\n",
    "            # for i in range(len(index2)-1):\n",
    "            #     if index2[i] + 1 == index2[i+1]:\n",
    "            #         pass\n",
    "            #     else:\n",
    "            #         end_index = index2[i]\n",
    "            # end_index = index2[-1]\n",
    "            closed_pos = all_points[all_points[:, -1] == 0.5]\n",
    "            max_height = np.max(closed_pos[:, 2])\n",
    "            position3 = np.where(all_points[:, 2] == max_height)[-1][0]\n",
    "            # position3 = np.where(all_points[:, 2] == 0.14)[0][-1]\n",
    "            waypoint3 = all_points[position3][:3]\n",
    "            # print(f\"Waypoint3: {waypoint3}\")\n",
    "            waypoint3 = np.array([-0.21113563, -0.33498523,  0.14742944])\n",
    "            extended_waypoint3 = np.tile(waypoint3, (position3 - position2, 1))\n",
    "            extended_waypoint[position2+1:position3+1] = extended_waypoint3\n",
    "\n",
    "\n",
    "            n_demo.create_dataset(\"waypoint\", data=extended_waypoint)\n",
    "\n",
    "            ## ----------------------- New mode key - 1 ----------------------------------- ##\n",
    "            mode = np.zeros((len(all_points), 1))\n",
    "            mode[position1:] = 1.0\n",
    "\n",
    "            n_demo.create_dataset(\"mode\", data=mode)\n",
    "\n",
    "\n",
    "\n",
    "print(\"New Dataset Created! Wohoooooo!!\")\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
